{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment — Node embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from time import time\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cora dataset EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cora dataset consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links. Let us take a closer look at this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/netspractice/network-science/main/datasets/cora_cites.txt'\n",
    "open('cora_cites.txt', 'wb').write(requests.get(url).content)\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/netspractice/network-science/main/datasets/cora_content.txt'\n",
    "open('cora_content.txt', 'wb').write(requests.get(url).content);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of nodes in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cora = nx.read_edgelist('cora_cites.txt')\n",
    "len(cora)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of nodes in a gigantic connected component (GCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcc_nodes = sorted(list(nx.connected_components(cora)), \n",
    "                   key=lambda x: len(x))[-1]\n",
    "gcc_cora = cora.subgraph(gcc_nodes).copy()\n",
    "len(gcc_cora)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nodes content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cora_content = pd.read_csv('cora_content.txt', sep='\t', \n",
    "                           header=None, index_col=0)\n",
    "cora_content.index = cora_content.index.astype('str')\n",
    "cora_content.index.name = 'node'\n",
    "cora_content.iloc[:5, :20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words. In this assignment, we will only work with categories and will not touch any information about words.\n",
    "\n",
    "Examples of node categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = cora_content.loc[gcc_nodes, [1434]]\n",
    "category = category.rename(columns={1434: 'category_name'})\n",
    "category.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Category distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(*np.unique(category, return_counts=True));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename categories to integer numbers (ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category['category_id'] = np.unique(category.category_name, \n",
    "                                    return_inverse = True)[1]\n",
    "category.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assortativity coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.set_node_attributes(gcc_cora, category.category_id.to_dict(), 'category')\n",
    "gcc_cora = nx.convert_node_labels_to_integers(gcc_cora)\n",
    "round(nx.attribute_assortativity_coefficient(gcc_cora, 'category'), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1. Node embedding visualization (0 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim==4.0.0\n",
    "!pip install karateclub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from karateclub import DeepWalk, Walklets, LaplacianEigenmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compare embedding algorithms that we saw in previous assignments: \n",
    "* Laplacian Eigenmaps\n",
    "* Truncated SVD of an adjacency matrix\n",
    "* DeepWalk\n",
    "* Walklets\n",
    "\n",
    "There is a usefull python package [*Karate Club*](https://github.com/benedekrozemberczki/karateclub) that contains implementations of these algorithms. Also we will use sklearn implementation of truncated SVD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding algorithms in Karate Club have a general interface\n",
    "\n",
    "```\n",
    "model.fit(graph)\n",
    "embedding = model.get_embedding()\n",
    "```\n",
    "\n",
    "However, the one inconvenient thing is that Walklets make a concatenation of Word2Vec embeddings to obtain final embeddings. In this way, the output dimensionality will be Word2Vec dimensionality multiplied by window size. A piece of source code:\n",
    "\n",
    "```python\n",
    "def get_embedding(self) -> np.array:\n",
    "    r\"\"\"Getting the node embedding.\n",
    "\n",
    "    Return types:\n",
    "        * **embedding** *(Numpy array)* - The embedding of nodes.\n",
    "    \"\"\"\n",
    "    return np.concatenate(self._embedding, axis=1)\n",
    "```\n",
    "\n",
    "Sometimes, it is usefull to have deep representation of nodes, but  we want to compare embeddings with the same dimensionality, so let us define our own class `MeanWalklets` that inherits `Walklets` and returns average embeddings in the `get_embedding` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c5393ffb3b8e11deee148cd111b3bf9e",
     "grade": false,
     "grade_id": "cell-1be4879cc8190e43",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class MeanWalklets(Walklets):\n",
    "    def get_embedding(self):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6facdeadc2ee2d369d082756713527bf",
     "grade": true,
     "grade_id": "cell-7c5dd45231ab30af",
     "locked": true,
     "points": 0.0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_model = MeanWalklets()\n",
    "test_model.fit(nx.karate_club_graph())\n",
    "test_emb = test_model.get_embedding()\n",
    "assert test_emb.shape == (34, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us visualize the 2d embeddings.\n",
    "\n",
    "Write a function `xy_embeddings` that takes a graph, compute 16d embeddings, reduce them into 2d via PCA and returns in the order:\n",
    "* Laplacian Eigenmaps\n",
    "* Truncated SVD of an adjacency matrix\n",
    "* DeepWalk\n",
    "* Walklets\n",
    "\n",
    "*Hints:*\n",
    "* *Suggested hyperparameters for DeepWalk and Walklets are `walk_number=10`, `walk_length=30`, `window_size=10`*\n",
    "* *You do not need reduce Walklets embeddgins to 2d, just directly use `PCAWalklets` with dimensionality 2*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bc41a75960ee98cb19af04de548100a0",
     "grade": false,
     "grade_id": "cell-d33f4faf41b0da0e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def xy_embeddings(_graph):\n",
    "    graph = _graph.copy()\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "026e746b06727b5c3c53c8a547e0659f",
     "grade": true,
     "grade_id": "cell-ce2531dac614957f",
     "locked": true,
     "points": 0.0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "laplacian_emb, svd_emb, deep_walk_emb, walklets_emb = xy_embeddings(gcc_cora)\n",
    "assert (deep_walk_emb.shape == laplacian_emb.shape \n",
    "        == svd_emb.shape == walklets_emb.shape == (2485, 2))\n",
    "assert -0.5 < laplacian_emb.min() < laplacian_emb.max() < 0.5\n",
    "assert svd_emb[0].sum() > 11\n",
    "assert round(abs(np.corrcoef(deep_walk_emb[:, 0], deep_walk_emb[:, 1])[0][1]), \n",
    "             2) == 0\n",
    "assert round(abs(np.corrcoef(walklets_emb[:, 0], walklets_emb[:, 1])[0][1]), \n",
    "             2) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, here we get a list of category ids to color data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_id = nx.get_node_attributes(gcc_cora, 'category')\n",
    "category_id = list(category_id.values())\n",
    "category_id[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6*4))\n",
    "cases = [[laplacian_emb, 'Laplacian Eigenmaps'], \n",
    "         [svd_emb, 'Truncated SVD'], \n",
    "         [deep_walk_emb, 'DeepWalk'], \n",
    "         [walklets_emb, 'Walklets']]\n",
    "for i, (emb, title) in enumerate(cases):\n",
    "    plt.subplot(4, 1, i+1)\n",
    "    plt.scatter(emb[:, 0], emb[:, 1], c=category_id, cmap=plt.cm.Set1, s=10)\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2. Test size dependency (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim==4.0.0\n",
    "!pip install karateclub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from karateclub import DeepWalk, Walklets, LaplacianEigenmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compare the quality of embedding algorithms on classification tasks depending on the size of test set.\n",
    "\n",
    "Write a function `embeddings_score` that takes a graph and computes 16d embeddigns, splits the dataset (X is embedding, y is category id) into train and test sets, fit `GradientBoostingClassifier` and returns a list of lists:\n",
    "* Micro-F1 score of Laplacian Eigenmaps for the test size 0.99, 0.95, 0.9, 0.8, 0.7\n",
    "* The same for Truncated SVD\n",
    "* The same for Deepwalk\n",
    "* The same for Walklets\n",
    "\n",
    "*Hints:* \n",
    "* *Use `train_test_split` splitting method from sklearn*\n",
    "* *Use `f1_score(y_test, y_pred, average='micro')` method from sklearn to calculate Micro-F1 score*\n",
    "* *It is ok if it takes about 2 minutes in Colab*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "92cd49c29464928c8eb6297ebb5473c0",
     "grade": false,
     "grade_id": "cell-0c29359bd3423c87",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def embeddings_score(_graph):\n",
    "    graph = _graph.copy()\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ce2cdad19bf2c5ae6dcfdbac55113e9",
     "grade": true,
     "grade_id": "cell-e2b5bae45ef6c5da",
     "locked": true,
     "points": 3.0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "scores = embeddings_score(gcc_cora)\n",
    "scores = np.array(scores)\n",
    "assert scores.shape == (4, 5)\n",
    "assert scores.mean() > 0.5\n",
    "X = np.stack([np.ones(5), np.arange(5)], axis=1)\n",
    "y = scores.mean(axis=0)\n",
    "assert (np.linalg.inv(X.T @ X) @ X.T @ y)[0] > 0.4\n",
    "mean_res = scores.mean(axis=1)\n",
    "assert mean_res[0] > mean_res[3] > mean_res[1] > mean_res[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "labels = ['Laplacian Eigenmaps', 'Truncated SVD', 'DeepWalk', 'Walklets']\n",
    "for i, score in enumerate(scores):\n",
    "    plt.plot([0.99, 0.95, 0.9, 0.8, 0.7], score, label=labels[i])\n",
    "    plt.scatter([0.99, 0.95, 0.9, 0.8, 0.7], score)\n",
    "plt.legend()\n",
    "plt.title('Embedding algorithms quality')\n",
    "plt.xlabel('Test set size')\n",
    "plt.ylabel('Micro-F1 score')\n",
    "plt.gca().invert_xaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3. Embedding dimensionality importance (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim==4.0.0\n",
    "!pip install karateclub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from karateclub import DeepWalk, Walklets, LaplacianEigenmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important property of the embedding is an ability to represent nodes in low-dimensional space. It will be great to quickly compress the most important and drop all excess imformation. Let us check how the dimensionality affects to quality and time cost.\n",
    "\n",
    "Write a function `embeddings_dim` that takes a graph, computes embeddings, splits dataset into train and test sets with test size 0.95, computes Micro-F1 scores, time costs and returns a tuple:\n",
    "* list of lists:\n",
    "  * Micro-F1 score for 8d Laplacian Eigenmaps, Truncated SVD, DeepWalk, Walklets\n",
    "  * The same for 16d\n",
    "  * The same for 32d\n",
    "  * The same for 64d\n",
    "  * The same for 128d\n",
    "* list of lists:\n",
    "  * Time cost (seconds) for 8d Laplacian Eigenmaps, Truncated SVD, DeepWalk, Walklets\n",
    "  * The same for 16d\n",
    "  * The same for 32d\n",
    "  * The same for 64d\n",
    "  * The same for 128d\n",
    "\n",
    "*Hints:*\n",
    "* *Use `time()` to get a current time moment*\n",
    "* *It is ok if it takes about 4 minutes in Colab*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8b9a4a6d0fac691fa412c69b9529bda3",
     "grade": false,
     "grade_id": "cell-e039ba7855f9792c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def embeddings_dim(_graph):\n",
    "    graph = _graph.copy()\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e1236e7cd77a3a9361d6c065e0cb58cb",
     "grade": true,
     "grade_id": "cell-60b4e70331597579",
     "locked": true,
     "points": 3.0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "scores, time_cost = embeddings_dim(gcc_cora)\n",
    "scores, time_cost = np.array(scores), np.array(time_cost)\n",
    "assert scores.shape == time_cost.shape == (5, 4)\n",
    "smean = scores.mean(axis=0)\n",
    "assert smean.argmin() == 2\n",
    "assert smean[1] < smean[0]\n",
    "assert smean[1] < smean[3]\n",
    "assert smean.mean() > 0.5\n",
    "tcmean = time_cost.mean(axis=0)\n",
    "assert tcmean.argmin() in [0, 1]\n",
    "assert tcmean.argmax() == 3\n",
    "assert time_cost[0, 0] < time_cost[-1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "labels = ['Laplacian Eigenmaps', 'Truncated SVD', 'DeepWalk', 'Walklets']\n",
    "for i, score in enumerate(scores.T):\n",
    "    plt.plot([8, 16, 32, 64, 128], score, label=labels[i])\n",
    "    plt.scatter([8, 16, 32, 64, 128], score)\n",
    "plt.legend()\n",
    "plt.xscale('log', base=2)\n",
    "plt.xlabel('Dimensionality')\n",
    "plt.ylabel('Micro-F1 score')\n",
    "plt.title('Dimensionality vs score')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "for i, cost in enumerate(time_cost.T):\n",
    "    plt.plot([8, 16, 32, 64, 128], cost, label=labels[i])\n",
    "    plt.scatter([8, 16, 32, 64, 128], cost)\n",
    "plt.legend()\n",
    "plt.xscale('log', base=2)\n",
    "plt.yscale('log', base=2)\n",
    "plt.xlabel('Dimensionality')\n",
    "plt.ylabel('Time cost (seconds)')\n",
    "plt.title('Dimensionality vs time cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4. Node2Vec model (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim==4.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, we will consider Node2Vec embedding algorithm. In Node2Vec, we learn a mapping of nodes to a low-dimensional space of features that maximizes the likelihood of preserving network neighborhoods of nodes. It is similar to the DeepWalk, but used *biased random walk procedure* which efficiently explores diverse neighborhoods. There are two parameters:\n",
    "\n",
    "* Return parameter $p$ controls the likelihood of immediately revisiting a node in the walk. Setting it to a high value ensures that we are less likely to sample an already-visited node in the following two steps.\n",
    "\n",
    "* In-out parameter $q$ allows the search to differentiate between “inward” and “outward” nodes. If $q > 1$, the random walk is biased towards nodes close to previous node. In contrast, if $q < 1$, the walk is more inclined to visit nodes which are further away from the previous node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://raw.githubusercontent.com/netspractice/network-science/main/images/node_embedding.png' width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we just moved from the node $t$ to $v$ and now we want to decide on the next step so it evaluates the transition probabilities on edges ($v$, $x$) leading from $v$. Then the *unnormalized* probability is\n",
    "\n",
    "$$\\alpha = \\begin{cases}\n",
    "\\frac{1}{p} &  \\text{ if } d_{tx} = 0\\\\\n",
    "1 & \\text{ if } d_{tx} = 1\\\\\n",
    "\\frac{1}{q} & \\text{ if } d_{tx} = 2\\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "where $d_{tx}$ is the shortest path distance between nodes $t$ and $x$. To compute the true probability, we need to normalize values so that the sum is 1.\n",
    "\n",
    "Write a function `biased_random_walk` that takes a graph, node for which we start random walk, length of walk, parameters `p` and `q` and returns a list with a random walk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c3969a9f986a4f1865096bf05e04db42",
     "grade": false,
     "grade_id": "cell-3e6acfc34a7f8627",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def biased_random_walk(G, node, path_length, p, q):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0da66e955cfde05f81a26df314d222af",
     "grade": true,
     "grade_id": "cell-146532f00986acf1",
     "locked": true,
     "points": 1.33,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "rw = biased_random_walk(gcc_cora, 0, 10, 0.1, 0.5)\n",
    "assert len(rw) == 10\n",
    "rw = biased_random_walk(gcc_cora, node=0, path_length=4, p=0.001, q=1000)\n",
    "assert rw[0] == rw[2]\n",
    "assert rw[1] == rw[3]\n",
    "rw = biased_random_walk(gcc_cora, node=0, path_length=4, p=1000, q=0.001)\n",
    "assert len(set(rw)) >= 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are auxiliary methods that generates biased random walks and encode this walks using Word2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "052df1277612b7f9ab114cc333045002",
     "grade": false,
     "grade_id": "cell-8a2e43b862b373da",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def biased_random_walks(G, walk_number, walk_length, p, q):\n",
    "    walks = []\n",
    "    for node in tqdm(G.nodes):\n",
    "        for _ in range(walk_number):\n",
    "            walk_from_node = biased_random_walk(G, node, walk_length, p, q)\n",
    "            walks.append(walk_from_node)\n",
    "    return np.array(walks)\n",
    "\n",
    "def node2vec_encode(G, walks, dimensions, window_size):\n",
    "    walks_str = walks.astype('str').tolist()\n",
    "    model = Word2Vec(walks_str, vector_size=dimensions, hs=1, sg=1, \n",
    "                     alpha=0.05, epochs=1, window=window_size, )\n",
    "    embedding = np.array([model.wv[str(n)] for n in range(len(G))])\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2bd9d1b2fc152f8ecd0f12a2ad3eb2d5",
     "grade": true,
     "grade_id": "cell-03d420988ac76488",
     "locked": true,
     "points": 1.33,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "rws = biased_random_walks(gcc_cora, walk_number=10, walk_length=30, p=0.25, q=0.5)\n",
    "emb = node2vec_encode(gcc_cora, walks=rws, dimensions=16, window_size=10)\n",
    "assert emb.shape == (2485, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `grid_search_pq`. Using 0.95 test set size, run the grid search for `GradientBoostingClassifier` and find the best values for $p, q \\in \\{0.25, 0.50, 1, 2, 4\\}$ with respect to Micro-F1 score. Return the best $p, q$.\n",
    "\n",
    "*Hints:*\n",
    "* *Suggested hyperparameters are `walk_number=10`, `walk_length=30`, `window_size=10`, `dimensions=16`*\n",
    "* *To pass time limits, calculate the best `p` and `q` and then rewrite the function as*\n",
    "\n",
    "```python\n",
    "def grid_search_pq(graph):\n",
    "    return best_p, best_q\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0422943692188f6966386fc93307449f",
     "grade": false,
     "grade_id": "cell-e01b1458086e8b5a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def grid_search_pq(graph):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1e98e3b9e5d1c1be59d46becfff8db28",
     "grade": true,
     "grade_id": "cell-d56cadb33744b38c",
     "locked": true,
     "points": 1.3399999999999999,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "best_p, best_q = grid_search_pq(gcc_cora)\n",
    "rws = biased_random_walks(gcc_cora, walk_number=10, walk_length=30, p=best_p, q=best_q)\n",
    "emb = node2vec_encode(gcc_cora, walks=rws, dimensions=16, window_size=10)\n",
    "category_id = nx.get_node_attributes(gcc_cora, 'category')\n",
    "category_id = list(category_id.values())\n",
    "X_train, X_test, y_train, y_test = train_test_split(emb, category_id, test_size=0.95, random_state=0)\n",
    "clf = GradientBoostingClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "assert f1_score(y_test, y_pred, average='micro') > 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "model = PCA(n_components=2)\n",
    "model.fit(emb)\n",
    "emb = model.transform(emb)\n",
    "plt.scatter(emb[:, 0], emb[:, 1], c=category_id, cmap=plt.cm.Set1, s=10)\n",
    "plt.title('Node2Vec')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
