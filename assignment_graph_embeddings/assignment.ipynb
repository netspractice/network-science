{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65fedf31",
   "metadata": {},
   "source": [
    "# Assignment — Graph embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d913448d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.cluster import k_means\n",
    "from scipy.sparse.linalg import svds\n",
    "import networkx as nx\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import torch.nn as nn\n",
    "from scipy.sparse import csr_matrix\n",
    "from torch.optim import Adam\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import mutual_info_score\n",
    "import torch\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dba3375",
   "metadata": {
    "id": "FZvmCa1k_nK-"
   },
   "source": [
    "In this assignment, we will evaluate node embedding methods on the facebook graph where nodes are pages and edges are links. Each node has a category: government, tv-show, company, politician."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dc3681",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-I2DhszTyVg3",
    "outputId": "19e0d089-5d3e-4e22-8671-ea29cd0ee92f"
   },
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/netspractice/network-science/main/datasets/musae_facebook_ego_802.gml'\n",
    "open('musae_facebook_ego_802.gml', 'wb').write(requests.get(url).content)\n",
    "G = nx.read_gml('musae_facebook_ego_802.gml')\n",
    "G = nx.convert_node_labels_to_integers(G)\n",
    "_labels = np.array(list(nx.get_node_attributes(G, 'value').values()))\n",
    "unique = list(set(_labels))\n",
    "labels = np.array([unique.index(l) for l in _labels])\n",
    "len(G), labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaa5be0",
   "metadata": {},
   "source": [
    "### Task 1. DeepWalk (0 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bed446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dbdbc7",
   "metadata": {
    "id": "VFEVAy9NmX1_"
   },
   "source": [
    "Deepwalk is an approach for learning latent representations of nodes in a network. The motivation of DeepWalk is based on an observation that the frequency of nodes occurrence in the short random walks in social networks is similar to the frequency of words occurrence in sentences in natural languages.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/netspractice/network-science/main/images/node_word_powerlaw.png\" width=600>\n",
    "\n",
    "The both follow a power-law distribution, therefore NLP word embedding models can help to represent nodes in networks. DeepWalk is based on SkipGram model that is trained to predict the context for a given word.\n",
    "\n",
    "<img src='https://lena-voita.github.io/resources/lectures/word_emb/w2v/window_prob1-min.png' width=600>\n",
    "\n",
    "(an image is taken from [NLP Course For You](https://lena-voita.github.io/nlp_course.html))\n",
    "\n",
    "DeepWalk uses nodes instead of words and random walks on a network instead of sentences. Let a central word be a start node of a random walk, context words be nodes in a tail of a random walk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e10c7b8",
   "metadata": {
    "id": "GNEIjHkD2Ess"
   },
   "source": [
    "Write a function `sample_random_walks` that takes a graph, number of walks per node and the length of walks, returns np.array of the shape (total number of walks) x (length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39830c4c",
   "metadata": {
    "deletable": false,
    "id": "9Ow1aAoq0N1V",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d3ce58f440fdc8e196671a9575dd819b",
     "grade": false,
     "grade_id": "cell-6dae7ab8bdd8bca1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def sample_random_walks(G, walks_per_node, length):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2bfe7e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "140ad69a04c642eebaed28a48c43c178",
      "0347a2a554574d57bd58b3cffbcf02d7",
      "a45fe0effeb14e0b9c65183329c85264",
      "c6d5b6e134f648da91d36226f7f35fd2",
      "858baf0e1f6c4dd58376cfad45f05b1d",
      "2b5df2606fc3442cb31ab7285c24e231",
      "7bc21924dbf2464784969d251c607146",
      "6caf612d17bb49acbd8312a29822f47f",
      "5a4b2899237741fcaebe44b8f082256b",
      "4ca75fe814e84d97894c9e6cbfc1fa10",
      "977bbb206542425da429a0d0d3b8c33c"
     ]
    },
    "deletable": false,
    "editable": false,
    "id": "ium8TB601-Ea",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bf580c164d7ab96868a44d09bf855390",
     "grade": true,
     "grade_id": "cell-6804ce30d354612f",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "bcec441e-3970-450f-9776-9df9055a8fe6"
   },
   "outputs": [],
   "source": [
    "walks_per_node = 5\n",
    "length = 5\n",
    "rwalks = sample_random_walks(G, walks_per_node, length)\n",
    "assert rwalks.shape == (len(G) * walks_per_node, length)\n",
    "assert np.all(rwalks[:, 0][::5] == np.arange(len(G)))\n",
    "A = nx.to_numpy_array(G)\n",
    "assert np.all(A[rwalks[0, :-1], rwalks[0, 1:]] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c22814",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ECHOTLkSbmaO",
    "outputId": "a4d8f720-0287-4790-a9b5-b1dd37f022bf"
   },
   "outputs": [],
   "source": [
    "rwalks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1a4ff6",
   "metadata": {
    "id": "FZH3kd2Tvvfh"
   },
   "source": [
    "Consider a graph $G=(V,E)$. Let the first node in each random walk be _start node_ and others be _context nodes_. DeepWalk uses two embedding layers:\n",
    "* $v_i: \\{0, 1\\}^{|V|} \\to \\mathbb R^{d}$ embeds the one-hot encoded vector of the start node $i$ to latent space\n",
    "* $u_j: \\{0, 1\\}^{|V|} \\to \\mathbb R^{d}$ embeds the one-hot encoded vector of the context node $j$ to latent space\n",
    "\n",
    "The objective is to maximize the probability that $i$ and $j$ co-occur on a random walk over the network. Maximizing of the probability is equivalent to minimizing negative log-likelihood:\n",
    "\n",
    "$$\\mathcal L = - \\frac{1}{|V|\\times N} \\sum_{i=1}^{|V|\\times N} \\sum_{j=1}^L \\log P(j|i)$$\n",
    "\n",
    "where $N$ is the number of walks per node, $L$ is the length of a random walk excluding start node. $P(j|i)$ can be modelled by softmax with dot product similarity score $\\text{sim}(i, j) = u_i^\\top v_j$ as follows:\n",
    "\n",
    "$$\\mathcal L = - \\frac{1}{|V|\\times N} \\sum_{i=1}^{|V|\\times N} \\sum_{j=1}^L \\log \\frac{\\exp(v_i^\\top u_j)}{\\sum_{k=1}^{|V|}\\exp(u_k^\\top v_i)}$$\n",
    "\n",
    "However, calculating $\\sum_{k=1}^{|V|}\\exp(v_i^\\top u_j)$ is computationally expensive in a large network. To overcome such an obstacle, we can approximate softmax by binary cross-entropy with _negative sampling_. Instead of calculating softmax, we draw some random (negative) context and minimize binary cross-entropy using sigmoid function:\n",
    "\n",
    "$$\\mathcal L^{\\text{pos}}_{ij} = -\\log P(j|i) = - \\log \\sigma(v_i^\\top u_j) \\\\\n",
    "\\mathcal L^\\text{neg}_{ij} = -\\sum_{k=1}^K \\log P(k|i) = -\\sum_{k=1}^K \\log (1 - \\sigma(v_i^\\top u_k)) \\\\ \n",
    "\\mathcal L = \\frac{1}{|V|\\times N} \\sum_{i=1}^{|V|\\times N} \\sum_{j=1}^L \\left(\\mathcal L^{\\text{pos}}_{ij} + \\mathcal L^\\text{neg}_{ij}\\right)$$\n",
    "\n",
    "where $K$ is the number of negative nodes for each context node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264d4a53",
   "metadata": {
    "id": "WWO3FS8f2BXA"
   },
   "source": [
    "Let us create a dataset for computing a such loss function. Write a class `NodeContextDataset`. \n",
    "\n",
    "Function `__init__` takes random walks, the number of nodes in a graph and the number of negative nodes per each context node.\n",
    "\n",
    "Function `__len__` returns the number of random walks.\n",
    "\n",
    "Function `__getitem__` takes an index of random walk, sample a negative context and returns a tuple:\n",
    "\n",
    "* start node, torch.int64\n",
    "* positive context nodes, torch.tensor of the shape (number of context nodes)\n",
    "* negative context nodes, torch.tensor of the shape (number of context nodes, number of negative nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bf6b7a",
   "metadata": {
    "deletable": false,
    "id": "65SsWhyu2SZd",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f37ac528f7f9c27b886cd738c11ed47f",
     "grade": false,
     "grade_id": "cell-92d80df6a92f6fc2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class NodeContextDataset(Dataset):\n",
    "    def __init__(self, rwalks, n_nodes, n_neg):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __len__(self):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b97c14",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "me3MM4yK4NSb",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "33e459957a271d9fbd1451657741be76",
     "grade": true,
     "grade_id": "cell-a745361ad8b8827a",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dataset = NodeContextDataset(rwalks, len(G), n_neg=5)\n",
    "assert len(dataset) == rwalks.shape[0]\n",
    "start_node, pos_context, neg_context0 = dataset[0]\n",
    "start_node, pos_context, neg_context1 = dataset[0]\n",
    "assert start_node == 0\n",
    "assert start_node.dtype == torch.int64\n",
    "assert start_node.shape == ()\n",
    "assert pos_context.shape == (4, )\n",
    "assert neg_context0.shape == neg_context1.shape ==(4, 5)\n",
    "assert not torch.all(neg_context0 == neg_context1)\n",
    "dloader = DataLoader(dataset, batch_size=2)\n",
    "for start_nodes, pos_context, neg_context in dloader:\n",
    "    break\n",
    "assert start_nodes.shape == (2,)\n",
    "assert pos_context.shape == (2, 4)\n",
    "assert neg_context.shape == (2, 4, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f787aa",
   "metadata": {
    "id": "XPaJWYeEOyH9"
   },
   "source": [
    "Write a function `cross_entropy` that takes vectors $v$, positive $u$, negative $u$ and returns the binary cross-entropy loss before reduction $\\frac{1}{|V|\\times N} \\sum_{i=1}^{|V|\\times N} \\sum_{j=1}^L$.\n",
    "\n",
    "_Remark: to prevent $-\\infty$ in log, add $\\varepsilon=1^{-6}$ as follows `torch.log(x + 1e-6)`_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b061c027",
   "metadata": {
    "deletable": false,
    "id": "Gc4yYrdtTysM",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8c52f1255915a57c9e9e136063d7a1bb",
     "grade": false,
     "grade_id": "cell-e2f4ba57342fd5e8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def cross_entropy(v, u_pos, u_neg):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0ddc41",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5cJcyDIqaUTL",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6560bbf935e37888e3e3b1bf47bd0881",
     "grade": true,
     "grade_id": "cell-588d17c4193a2d02",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "start_node_emb = torch.randn(len(G), 16)\n",
    "context_emb = torch.randn(len(G), 16)\n",
    "v = start_node_emb[start_nodes]\n",
    "u_pos = context_emb[pos_context]\n",
    "u_neg = context_emb[neg_context]\n",
    "loss = cross_entropy(v, u_pos, u_neg)\n",
    "assert loss.shape == (2, 4)\n",
    "lpos = -torch.log(torch.sigmoid(v[0] @ u_pos[0, 0]) + 1e-6)\n",
    "lneg = -torch.log(1 - torch.sigmoid(torch.tensor([v[0] @ u_neg[0, 0, i] for i in range(5)])) + 1e-6).sum()\n",
    "assert round((lpos + lneg).item(), 2) == round(loss[0, 0].item(), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99e2494",
   "metadata": {
    "id": "gH1kaJd4Sccq"
   },
   "source": [
    "Here is SkipGram model with negative sampling. It takes start nodes and positive, negative context nodes, returns cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10de52e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "_nAOYIwr8_ZQ",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "53769d5cafb453b1bcf37472d77b6fa5",
     "grade": false,
     "grade_id": "cell-79e82ef853f9c439",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, n_nodes, dim):\n",
    "        super().__init__()\n",
    "        self.start_node_emb = nn.Embedding(n_nodes, dim)\n",
    "        self.context_emb = nn.Embedding(n_nodes, dim)\n",
    "    def forward(self, start_nodes, pos_context, neg_context):\n",
    "        v = self.start_node_emb(start_nodes)\n",
    "        u_pos = self.context_emb(pos_context)\n",
    "        u_neg = self.context_emb(neg_context)\n",
    "        return cross_entropy(v, u_pos, u_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a37b0d",
   "metadata": {
    "id": "3YDBr9IfdMc_"
   },
   "source": [
    "Let us train the model using Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38c206b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297,
     "referenced_widgets": [
      "b0d203535ccc4f41b7fb67691facbefb",
      "1c827ad658e04c299d037bc59beebef1",
      "de0f72586c9d4012a88b659c86c81cbf",
      "30f2e55aef0343389c4ca896954a65d7",
      "f283f9347e664f05994b4594b090fd60",
      "d8276173d44f4f9e93b16416be64e56a",
      "3cf1612457ef404fb490bfcef697ddfa",
      "d622a5a1246b494cae59fa22e43290fb",
      "6e6844492ccb4ac2a79b3045df5a94ad",
      "eb55b760369d4215885394806b05d3c3",
      "c56b3c190ee54116bb99ddbe924007d8"
     ]
    },
    "id": "0CnMaY-iBNot",
    "outputId": "3e5b7d38-de81-46e5-b1ff-1507ba9f456b"
   },
   "outputs": [],
   "source": [
    "sgmodel = SkipGram(n_nodes=len(G), dim=16)\n",
    "epoch_loss = []\n",
    "opt = Adam(sgmodel.parameters(), lr=0.1)\n",
    "dloader = DataLoader(dataset, batch_size=len(G))\n",
    "for epoch in trange(50):\n",
    "    for start_nodes, pos_context, neg_context in dloader:\n",
    "        loss = sgmodel(start_nodes, pos_context, neg_context).sum(dim=1).mean()\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    epoch_loss.append(loss.item())\n",
    "plt.plot(epoch_loss);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6a110e",
   "metadata": {
    "id": "fOEAkiCUUfsu"
   },
   "source": [
    "We evaluate the model by mutual information between ground truth labels and cluster indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e834dee",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "1mdPsAiWFZ0Y",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95d47fc2fc4c52f43987f446abb71cb2",
     "grade": true,
     "grade_id": "cell-1685a473a18d47f8",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    emb = sgmodel.start_node_emb(torch.arange(len(G)))\n",
    "_, pred_labels, _ = k_means(emb, n_clusters=8)\n",
    "mi = mutual_info_score(labels, pred_labels)\n",
    "assert mi > 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9659552",
   "metadata": {
    "id": "As_bLu2mVGae"
   },
   "source": [
    "Let us plot the t-SNE visualization of node embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb479b52",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 499
    },
    "id": "BrZKP5i3UzFs",
    "outputId": "b9675b64-eb20-43cf-f7ca-c0ed59fa5124"
   },
   "outputs": [],
   "source": [
    "decomposition = TSNE(n_components=2)\n",
    "xy_emb = decomposition.fit_transform(emb)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "scatter = plt.scatter(xy_emb[:, 0], xy_emb[:, 1], c=labels, s=10, cmap=plt.cm.Set2)\n",
    "\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=unique)\n",
    "plt.title(f'MI: {mi:.4f}');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7934862",
   "metadata": {},
   "source": [
    "### Task 2. Node2Vec (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4777316",
   "metadata": {
    "id": "5q-lgag_YnQ3"
   },
   "source": [
    "In this task, we will consider Node2Vec embedding algorithm. In Node2Vec, we learn a mapping of nodes to a low-dimensional space of features that maximizes the likelihood of preserving network neighborhoods of nodes. It is similar to DeepWalk, but uses *biased random walk procedure* which efficiently explores diverse neighborhoods. There are two parameters:\n",
    "\n",
    "* Return parameter $p$ controls the likelihood of immediately revisiting a node in the walk. Setting it to a high value ensures that we are less likely to sample an already-visited node in the following two steps.\n",
    "\n",
    "* In-out parameter $q$ allows the search to differentiate between “inward” and “outward” nodes. If $q > 1$, the random walk is biased towards nodes close to previous node. In contrast, if $q < 1$, the walk is more inclined to visit nodes which are further away from the previous node.\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/netspractice/network-science/main/images/biased_random_walk.png' width=300>\n",
    "\n",
    "Consider that we just moved from the node $t$ to $v$ and now we want to decide on the next step so it evaluates the transition probabilities on edges ($v$, $x$) leading from $v$. Then the *unnormalized* probability is\n",
    "\n",
    "$$\\alpha = \\begin{cases}\n",
    "\\frac{1}{p} &  \\text{ if } d_{tx} = 0\\\\\n",
    "1 & \\text{ if } d_{tx} = 1\\\\\n",
    "\\frac{1}{q} & \\text{ if } d_{tx} = 2\\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "where $d_{tx}$ is the shortest path distance between nodes $t$ and $x$. To compute the true probability, we need to normalize values so that the sum is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04677dca",
   "metadata": {
    "id": "9LFfr6fy18Gb"
   },
   "source": [
    "Write a function `biased_random_walk` that takes a graph, node for which we start random walk, length of walk, parameters `p` and `q` and returns a list with random walks.\n",
    "\n",
    "*Hint: do not use `nx.shortest_path` to calcule shortest paths, it is too expensive. Look at the image above — we can explicitly calculate probabilities for all neighbors of $v$ using neighborhood of $t$*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d00803",
   "metadata": {
    "deletable": false,
    "id": "HGXAsOHw-aUV",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "248810a536b6d7a59e0b93a8937cf7a8",
     "grade": false,
     "grade_id": "cell-982226ce9aa30f91",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def biased_random_walk(G, node, length, p, q):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e67494",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "uzi9dGUUcYMh",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2435b88e56db99f5401d6df7bd8aac19",
     "grade": true,
     "grade_id": "cell-0404594e2e4226e7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "rwalks = biased_random_walk(G, 0, 10, 0.1, 0.5)\n",
    "assert len(rwalks) == 10\n",
    "rwalks = biased_random_walk(G, node=0, length=4, p=0.001, q=1000)\n",
    "assert rwalks[0] == rwalks[2]\n",
    "assert rwalks[1] == rwalks[3]\n",
    "rwalks = biased_random_walk(G, node=0, length=4, p=1000, q=0.001)\n",
    "assert len(set(rwalks)) >= 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8175c64d",
   "metadata": {
    "id": "LGqNp_axcyFk"
   },
   "source": [
    "Let us generate biased random walks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abeb314",
   "metadata": {
    "id": "cmys-WiMGIAz"
   },
   "outputs": [],
   "source": [
    "def sample_biased_random_walks(G, walks_per_node, length, p, q):\n",
    "    walks = []\n",
    "    for node in tqdm(G.nodes, leave=False, desc='Sampling biased random walks'):\n",
    "        for _ in range(walks_per_node):\n",
    "            walk_from_node = biased_random_walk(G, node, length, p, q)\n",
    "            walks.append(walk_from_node)\n",
    "    return np.array(walks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535d7e2c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142,
     "referenced_widgets": [
      "cdf30dfa2936467b807ed565733c1921",
      "a5bcacf8222c44faa916edc17f0a1192",
      "6042e824f60d4eab84af0f54b22be1f1",
      "fd5b890b765440538c08f80a99da2155",
      "d49c98f782bd4c79a2424303a3dc0c00",
      "e466c6e63edd43f5a615be8742907054",
      "5aa483f387d347c09d05904a8ccc7ac0",
      "cd4342fc551d4e4fb7aef40b5c2b6ec3",
      "bd3ebfe59de94b18be81439cf63fd9c3",
      "3dddb00aca4347c4a20f2cc5fa58e6b0",
      "81f258b6d3034bf28c7e12fdc314bcb5"
     ]
    },
    "id": "z5GQKyl3GXEy",
    "outputId": "852e0c93-1615-464e-82ad-d0ebb0f46d7c"
   },
   "outputs": [],
   "source": [
    "walks_per_node = 5\n",
    "length = 5\n",
    "rwalks = sample_biased_random_walks(G, walks_per_node, length, p=0.25, q=0.25)\n",
    "rwalks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d289d4a",
   "metadata": {
    "id": "MhZic6vYQbs3"
   },
   "source": [
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51831b79",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297,
     "referenced_widgets": [
      "914a2510a07e4004b13764468819c080",
      "c2e03743985842eb80d5a2442bf75164",
      "a9b7867beb84430db0f2552555e80b43",
      "270162e6b6a84169b9cd640027b45efa",
      "f7a84d90f27c497a8d5c75710d27402b",
      "b6d3f3e387464b50bc6037efbfb7e8d3",
      "c90a1ed5bf53491f9cddf3ffec0b6ae9",
      "2fc47ae41520477182d42c42b453e192",
      "1b15e7e2ce7d4331adc7730e2c9145a8",
      "359ffbde40634781bd831aee0ca559e9",
      "bcffc8aad49949b89d0e708dc69b72dc"
     ]
    },
    "id": "EQnnsFnwGlGz",
    "outputId": "47a47314-9cc2-448e-92a9-9a4a6b867d94"
   },
   "outputs": [],
   "source": [
    "e_loss = []\n",
    "dataset = NodeContextDataset(rwalks, len(G), n_neg=5)\n",
    "dloader = DataLoader(dataset, batch_size=len(G))\n",
    "sgmodel = SkipGram(n_nodes=len(G), dim=16)\n",
    "opt = Adam(sgmodel.parameters(), lr=0.1)\n",
    "for e in trange(50):\n",
    "    for start_nodes, pos_context, neg_context in dloader:\n",
    "        loss = sgmodel(start_nodes, pos_context, neg_context).sum(dim=1).mean()\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    e_loss.append(loss.item())\n",
    "plt.plot(e_loss);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8e8b11",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "MUkb4k-mdsj2",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7005fe5ef24ada019070d2e796d90abd",
     "grade": true,
     "grade_id": "cell-d1d23ef4a88759f7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    emb = sgmodel.start_node_emb(torch.arange(len(G)))\n",
    "_, pred_labels, _ = k_means(emb, n_clusters=8)\n",
    "mi = mutual_info_score(labels, pred_labels)\n",
    "assert mi > 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5328017",
   "metadata": {
    "id": "3J8GA322eAtB"
   },
   "source": [
    "Find the best $p$ and $q$. Write a function `grid_search_pq` that takes a graph, node labels and returns the best $p$ and $q$ by grid search in $\\{0.01, 0.2, 1, 2, 8\\}$. Sample biased random walks with 5 walks per node and the length 5. Evaluate the model by mutual information score between ground truth labels and k-means with 8 clusters. To pass time limits, calculate the best `p` and `q` and then rewrite the function as\n",
    "\n",
    "```python\n",
    "def grid_search_pq(graph):\n",
    "    return best_p, best_q\n",
    "    # your grid search implementation ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eb0722",
   "metadata": {
    "deletable": false,
    "id": "drn8aJkje0D1",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2381e148a50dffea83ac5792805d7c31",
     "grade": false,
     "grade_id": "cell-63bf0f1902240663",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def grid_search_pq(G, labels):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a5c226",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "425242b29e7f447dabfa9c485ed79ae0",
      "f5407390b4d542f89e20d9d0ef283f81",
      "16ddafa3b1444ab588aae9b2c60a5b9f",
      "4c30bbca485c410eb1f9f47f601761d4",
      "16826c81b2214152ab4c6e1c08384bf3",
      "493277c7e57c44c89218572563b0b163",
      "96e7b0f031ee42fa8e82e379520bf485",
      "69fec3b362de42eca1f1995cca3582d6",
      "d1a021c04e824783b170e6308d364b63",
      "dda702789d244845a51cc77cc548ae4b",
      "0b4526e7de6a4370b2e6dd46210b01f4",
      "c43fa70121f9421dad2ccd42a68634dd",
      "df43e816b3ec4793b507a1506cd027eb",
      "5a7cc0fc092d47d085e5226139f57c55",
      "87b007410dc24c42ac6beb66423fd340",
      "7e9d3438ae3949baa29f0349ca859661",
      "13d4f9f6410a40aeba8d49e11ff5f8ba",
      "f0428256bacd42a4af9207128268844e",
      "80b0a08ffb6c47e898224f6e1bf65672",
      "52bfec89b8e24cd58213d5a3370f3dc6",
      "15c83946d3324f4eb84419a6b7e7c352",
      "6876ce2af20f4477b3f4093579456d52"
     ]
    },
    "deletable": false,
    "editable": false,
    "id": "igTGn7GALOJN",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "13bc7faca7c1ba4b90f008cd8a913296",
     "grade": true,
     "grade_id": "cell-6102dd78a7156238",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "2ac286aa-67c8-43ee-ff8e-fff6ba43faef"
   },
   "outputs": [],
   "source": [
    "best_p, best_q = grid_search_pq(G, labels)\n",
    "rwalks = sample_biased_random_walks(G, 5, 5, best_p, best_q)\n",
    "dataset = NodeContextDataset(rwalks, len(G), n_neg=5)\n",
    "dloader = DataLoader(dataset, batch_size=len(G))\n",
    "sgmodel = SkipGram(n_nodes=len(G), dim=16)\n",
    "opt = Adam(sgmodel.parameters(), lr=0.1)\n",
    "dloader = DataLoader(dataset, batch_size=len(G))\n",
    "for epoch in trange(50):\n",
    "    for start_nodes, pos_context, neg_context in dloader:\n",
    "        loss = sgmodel(start_nodes, pos_context, neg_context).sum(dim=1).mean()\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "with torch.no_grad():\n",
    "    emb = sgmodel.start_node_emb(torch.arange(len(G)))\n",
    "_, pred_labels, _ = k_means(emb, n_clusters=8)\n",
    "mi = mutual_info_score(labels, pred_labels)\n",
    "assert mi > 0.21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1105e5ca",
   "metadata": {
    "id": "4diPYQKCMdoT"
   },
   "source": [
    "Plot t-SNE visualization of node embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee44d6d6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 499
    },
    "id": "qkzOkF3NGzbR",
    "outputId": "36fc554c-d7f8-488c-8e09-c964dc16b540"
   },
   "outputs": [],
   "source": [
    "decomposition = TSNE(n_components=2)\n",
    "xy_emb = decomposition.fit_transform(emb)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "scatter = plt.scatter(xy_emb[:, 0], xy_emb[:, 1], c=labels, s=10, cmap=plt.cm.Set2)\n",
    "\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=unique)\n",
    "plt.title(f'MI: {mi:.4f}');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d2e67a",
   "metadata": {},
   "source": [
    "### Task 3. Hierarchical softmax (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0098e06b",
   "metadata": {
    "id": "I_93vIz0nsfT"
   },
   "source": [
    "Hierarchical softmax is an alternative way to approximate softmax in DeepWalk and similar models such as Walklets, Node2Vec. Remind the regular loss function with softmax:\n",
    "\n",
    "$$\\mathcal L = - \\frac{1}{|V|\\times N} \\sum_{i=1}^{|V|\\times N} \\sum_{j=1}^L \\log P(j|i) = - \\frac{1}{|V|\\times N} \\sum_{i=1}^{|V|\\times N} \\sum_{j=1}^L \\log \\frac{\\exp(v_i^\\top u_j)}{\\sum_{k=1}^{|V|}\\exp(v_i^\\top u_k)}$$\n",
    "\n",
    "We have observed that computing $\\sum_{k=1}^{|V|}\\exp(v_i^\\top u_k)$ is expensive in a large network. Indeed, it takes $O(|V|)$ since we need to compute $v_i^\\top u_k$ for each $k$. \n",
    "\n",
    "Hierarchical softmax allows to approximate softmax by propagation probabilities in a binary tree where leaves are nodes in the original network. Consider a simple network on 6 nodes and the corresponding binary tree.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/netspractice/network-science/main/images/hs_btree.png\" width=600>\n",
    "\n",
    "We observe that leaves correspond to nodes in the network: the leaf 5 corresponds to the node 0, the leaf 6 corresponds to the node 1 and so on. Let us call all other white nodes in the binary tree units.\n",
    "\n",
    "Write a function `get_binary_tree` that takes number of leaves and returns a binary tree.\n",
    "\n",
    "_Hint: create a binary tree by `nx.balanced_tree(2, depth, nx.DiGraph)` and drop extra leaves. The depth of the tree can be calculated as $\\lceil \\log_2 |V| \\rceil$._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8115fe",
   "metadata": {
    "deletable": false,
    "id": "TTpjUbSMjORQ",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "01c8b6fa4bd256a22b699e36c1556bbd",
     "grade": false,
     "grade_id": "cell-fa6e506ce6741f45",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_binary_tree(n_leaves):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49c2c6d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "VinKbEWplrvy",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8fcd40389d09de393cba575495e2513d",
     "grade": true,
     "grade_id": "cell-12931d335d97bb1c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "btree = get_binary_tree(n_leaves=len(G))\n",
    "leaves = [n for n in btree.nodes if btree.degree[n] == 1]\n",
    "assert len(leaves) == len(G)\n",
    "assert list(btree.nodes)[0] == 0\n",
    "assert nx.is_weakly_connected(btree)\n",
    "assert btree.in_degree[0] == 0\n",
    "assert np.all([btree.in_degree[n] == 1 for n in range(1, len(btree))])\n",
    "assert np.all([btree.out_degree[n] == 2 for n in range(len(btree) - len(leaves))])\n",
    "assert np.all([btree.out_degree[n] == 0 for n in range(len(btree) - len(leaves), len(btree))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17cb1eb",
   "metadata": {
    "id": "8KVUtUtm8OIy"
   },
   "source": [
    "If we assign a probability conditioned by the start node for each unit, we can calculate a probability of each context node as the product of probabilities of all units in the path from the root unit to a corresponding leaf taking $p_i$ if we go to the left and $1 - p_i$ if we go to the right from the unit $i$. For example, consider a path from the root unit to the context node 5 (leaf 10):\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/netspractice/network-science/main/images/hs_btree_proba.png\" width=600>\n",
    "\n",
    "$$P(5|i) = p_0\\cdot (1-p_1) \\cdot (1-p_4)$$\n",
    "\n",
    "We can observe following thigs:\n",
    "* Each unit divides probability space into two subspace, so that probabilities on leaves are summed up to 1.\n",
    "* We make only 3 multiplucations instead of 6 summations in regular softmax. Indeed, the number of multiplications is equal to the depth of the binary tree $O(\\log_2|V|)$. The number of summations in regular softmax is $O(|V|)$. That is, the cost of calculating probabilities for all context nodes is reduced from $O(|V|^2)$ to $O(|V|\\log_2|V|)$.\n",
    "\n",
    "We condition probabilities by the start node as follows:\n",
    "\n",
    "$$p_j(i) = \\sigma(v_i^\\top u_j)$$\n",
    "\n",
    "where $v_i$ is embedding of the start node $i$ and $u_j$ is embedding of the unit $j$. Thus, we have no embeddings for context nodes in this model. Note that $1-\\sigma(v_i^\\top u_j)$ can be substituted to $\\sigma(-v_i^\\top u_j)$. Let us call a sign under the sigmoid a turn $t_k$: $-1$ if we go to the left and $1$ if we go to the right. Thus the probability of co-occurance of the context node 5 and the start node, say, 1 is\n",
    "\n",
    "$$P(5|1) = \\sigma(t_0 v_1^\\top u_0) \\cdot \\sigma(t_1 v_1^\\top u_1) \\cdot \\sigma(t_2 v_1^\\top u_4)$$\n",
    "\n",
    "where $t_0=1$, $t_1=-1$, $t_2=-1$. Our objective is to maximize the probability of co-occurance of start nodes and context nodes by minimizing the negative log-likelihood, thus the loss function is turned out to\n",
    "\n",
    "$$\\mathcal L = - \\frac{1}{|V|\\times N} \\sum_{i=1}^{|V|\\times N} \\sum_{j=1}^L \\sum_{k=1}^K\\log \\sigma(t_{[jk]} v_i^\\top u_{[jk]})$$\n",
    "\n",
    "where $K$ is the path length from the root to the leaf excluding the leaf. Subscript $[jk]$ means the index corresponded to k-th step of the path to j-th context node. Note that the path length depends on a leaf, it makes calculating loss in mini-batch training inconvenient, so let us set $K$ as the depth of the tree and multiply the loss by _training mask_ $z_{jk}$: $0$ for all $k$ that greater than path length and $1$ otherwise. The final loss function is\n",
    "\n",
    "$$\\mathcal L = - \\frac{1}{|V|\\times N} \\sum_{i=1}^{|V|\\times N} \\sum_{j=1}^L \\sum_{k=1}^K\\log \\sigma(t_{[jk]} v_i^\\top u_{[jk]}) z_{[jk]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d59f742",
   "metadata": {
    "id": "lk8UBAd214CG"
   },
   "source": [
    "Let us create a dataset for computing a such loss function. Write a class `NodePathDataset`. \n",
    "\n",
    "Function `__init__` takes random walks, binary tree, depth of the binary tree and finds paths for each leaf.\n",
    "\n",
    "Function `__len__` returns the number of random walks.\n",
    "\n",
    "Function `__getitem__` takes an index of random walk and returns a tuple:\n",
    "\n",
    "* start node, torch.int64\n",
    "* context nodes, torch.tensor of the shape (number of context nodes)\n",
    "* paths to the context nodes excluding a leaf: torch.tensor of the shape (number of context nodes, depth). Padded with zeros if the depth is greater than the path length.\n",
    "* turns on the paths: torch.tensor of the shape (number of context nodes, depth). Padded with zeros if the depth is greater than the path length. Define the left turn as a link to the even unit.\n",
    "* training mask on the paths: torch.tensor of the shape (number of context nodes, depth)\n",
    "\n",
    "_Hint: use `nx.shortest_path(btree, 0, n)` to find a path from the root unit to the leaf `n` in the tree._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34018cf5",
   "metadata": {
    "deletable": false,
    "id": "gNmgoZW1l9Wg",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c0cc9935a2395e5bc227fbeb197535d8",
     "grade": false,
     "grade_id": "cell-48de35ab799a5afd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class NodePathDataset(Dataset):\n",
    "    def __init__(self, rwalks, btree, depth):\n",
    "        self.start_nodes = torch.tensor(rwalks[:, 0])\n",
    "        self.context = torch.tensor(rwalks[:, 1:])\n",
    "        self.paths = []\n",
    "        self.turns = []\n",
    "        self.masks = []\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        self.paths = torch.tensor(self.paths)\n",
    "        self.turns = torch.tensor(self.turns)\n",
    "        self.masks = torch.tensor(self.masks)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.start_nodes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.start_nodes[idx], \n",
    "            self.context[idx], \n",
    "            self.paths[self.context[idx]], \n",
    "            self.turns[self.context[idx]], \n",
    "            self.masks[self.context[idx]]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b324eb98",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "mColU52_WOJi",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d8ce7f617444f4b2c156cc851907e46d",
     "grade": true,
     "grade_id": "cell-41adebff43c8f34d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "depth = 12\n",
    "dataset = NodePathDataset(rwalks, btree, depth)\n",
    "start_node, context, paths, turns, masks = dataset[0]\n",
    "assert start_node.shape == ()\n",
    "assert context.shape == (4,)\n",
    "assert paths.shape == turns.shape == masks.shape == (4, 12)\n",
    "assert start_node == 0\n",
    "leaves = [n for n in btree.nodes if btree.degree[n] == 1]\n",
    "shortest_path = nx.shortest_path(btree, 0, leaves[context[1]])[:-1]\n",
    "mask = [1] * len(shortest_path)\n",
    "if len(shortest_path) < depth:\n",
    "    n_extra_units = depth - len(shortest_path)\n",
    "    shortest_path = shortest_path + [0] * n_extra_units\n",
    "    mask = mask + [0] * n_extra_units\n",
    "assert np.all(shortest_path == paths[1].numpy())\n",
    "assert np.all(mask == masks[1].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6321a0c1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A66EbScvpz6z",
    "outputId": "af4e60e6-87b0-4fca-aba1-1a5afb87bd28"
   },
   "outputs": [],
   "source": [
    "dloader = DataLoader(dataset, batch_size=2)\n",
    "for start_nodes, context, paths, turns, masks in dloader:\n",
    "    break\n",
    "start_nodes.shape, context.shape, paths.shape, turns.shape, masks.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1e3e06",
   "metadata": {
    "id": "eoXn4BU1oEBd"
   },
   "source": [
    "Write a function `hierarchical_softmax_loss` that takes vectors $v$, $u$ and returns the hierarchical softmax loss before reduction $\\frac{1}{|V|\\times N} \\sum_{i=1}^{|V|\\times N} \\sum_{j=1}^L$.\n",
    "\n",
    "_Remark: to prevent $-\\infty$ in log, add $\\varepsilon=1^{-6}$ as follows `torch.log(x + 1e-6)`_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a41aa6",
   "metadata": {
    "deletable": false,
    "id": "0haP3i7hni4c",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2d24227f9a5b502d535b0beff12e17f1",
     "grade": false,
     "grade_id": "cell-5580ea00c3649ace",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def hierarchical_softmax_loss(v, u, turns, masks):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7207d64d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "vf0XAmFQpVhP",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "77571345c5f803b3d52b802c082392e0",
     "grade": true,
     "grade_id": "cell-8c8320b2c7c030eb",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "start_node_emb = torch.randn(len(G), 16)\n",
    "path_emb = torch.randn(len(G) - 1, 16)\n",
    "v = start_node_emb[start_nodes]\n",
    "u = path_emb[paths]\n",
    "loss = hierarchical_softmax_loss(v, u, turns, masks)\n",
    "assert loss.shape == (2, 4)\n",
    "l = -torch.log(torch.sigmoid(torch.tensor([v[0] @ u[0, 0, i] * turns[0, 0, i] for i in range(12)])) + 1e-6) * masks[0, 0]\n",
    "assert round(l.sum().item(), 2) == round(loss[0, 0].item(), 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cedfdd",
   "metadata": {
    "id": "KrmcWlWDxyjm"
   },
   "source": [
    "Here is SkipGram model with hierarchical softmax. The number of vectors in `path_emb` is `n_nodes - 1`, since it can be shown that the number of units is equal to $|V|-1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a099d86",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "gfQ3aA-Bvxer",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8535c3166b0b7c5bc2e57ab2960ddc75",
     "grade": false,
     "grade_id": "cell-39c948726656b5ba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class SkipGramHierarchicalSoftmax(nn.Module):\n",
    "    def __init__(self, n_nodes, dim):\n",
    "        super().__init__()\n",
    "        self.start_node_emb = nn.Embedding(n_nodes, dim)\n",
    "        self.path_emb = nn.Embedding(n_nodes - 1, dim)\n",
    "    def forward(self, start_nodes, paths, turns, masks):\n",
    "        v = self.start_node_emb(start_nodes)\n",
    "        u = self.path_emb(paths)\n",
    "        loss = hierarchical_softmax_loss(v, u, turns, masks)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf4a130",
   "metadata": {
    "id": "uJgoNM6qz3ra"
   },
   "source": [
    "Train the model using Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64549ec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297,
     "referenced_widgets": [
      "a47c5058d8884d9bbc530cf0d90c852a",
      "9c92b0490a8a47559ca2e4e77c298ad0",
      "a88abf77fa3d47edafc5920e8652c626",
      "8125083ea38348b590dc5b4fb01cfa75",
      "611f178f0f2a4159bfdc01c8a9efdc53",
      "b6c0b404398249fd98fa0afbabbcd390",
      "dfef0cc5de4e42d096e5db441e750fa1",
      "5d8a3486126a4b5087a30cbe19daf9de",
      "983e06bfe9074936a23d68355b90840f",
      "8dc0e04c1c9c4a79ab4780cff7fa6588",
      "3df3ed05b1c94c8ead1039797aaf2855"
     ]
    },
    "id": "qyUFLJnx1npz",
    "outputId": "848b4d4b-c62e-498f-9127-7f0f5ba6d2b7"
   },
   "outputs": [],
   "source": [
    "sgmodel = SkipGramHierarchicalSoftmax(n_nodes=len(G), dim=16)\n",
    "epoch_loss = []\n",
    "opt = Adam(sgmodel.parameters(), lr=0.1)\n",
    "dloader = DataLoader(dataset, batch_size=len(G))\n",
    "for epoch in trange(50):\n",
    "    for start_nodes, context, paths, turns, masks in dloader:\n",
    "        loss = sgmodel(start_nodes, paths, turns, masks).sum(dim=1).mean()\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    epoch_loss.append(loss.item())\n",
    "plt.plot(epoch_loss);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9bb721",
   "metadata": {
    "id": "jFwKNMO3z8X2"
   },
   "source": [
    "Evaluate the model by mutual information between ground truth labels and cluster indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9a4407",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Y0ugT6Ep173h",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "163de70b349c978e9503141a12c90a5e",
     "grade": true,
     "grade_id": "cell-736c8f9bf1ffccd8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    emb = sgmodel.start_node_emb(torch.arange(len(G)))\n",
    "_, pred_labels, _ = k_means(emb, n_clusters=8)\n",
    "mi = mutual_info_score(labels, pred_labels)\n",
    "assert mi > 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c99ec0",
   "metadata": {
    "id": "cPql7hMu0HF8"
   },
   "source": [
    "Plot t-SNE visualization of node embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a96955",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 499
    },
    "id": "dZnRfklrzFsx",
    "outputId": "0e9e92ec-b26e-4b89-8088-9afef0dbb224"
   },
   "outputs": [],
   "source": [
    "decomposition = TSNE(n_components=2)\n",
    "xy_emb = decomposition.fit_transform(emb)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "scatter = plt.scatter(xy_emb[:, 0], xy_emb[:, 1], c=labels, s=10, cmap=plt.cm.Set2)\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=unique)\n",
    "plt.title(f'MI: {mi:.4f}');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89619336",
   "metadata": {
    "id": "lCmzKtCAJ-zD"
   },
   "source": [
    "The hierarchical softmax can be improved by building a binary tree where hubs are closer to the root unit. In this task we have used the randomly builded binary tree and we can see that it works pretty well with respect to negative sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244053d5",
   "metadata": {},
   "source": [
    "### Task 4. GraRep (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cad8e0",
   "metadata": {
    "id": "aviJdvivWW6D"
   },
   "source": [
    "The idea behind GraRep is to account for multiscale relationships between nodes. For example,a student can have 1-hop neighborhood of friends, 2-hop neighborhood of classmates, 3-hop neighborhood of society and so on.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/netspractice/network-science/main/images/multiscale_relationships.png\" width=350>\n",
    "\n",
    "Unlike DeepWalk, GraRep defines different objective functions for capturing the different $k$-step local relational information by manipulating transition matrices in the $k$-th powers defined over the graph. Recall that the transition matrix is $P = D^{-1}A$ where $D$ is a degree matrix. $P_{ij}$ refers to a probability to move from the node $i$ to the node $j$ in one step of a random walk. Therefore, $P^k_{ij}$ is a probability to move from the node $i$ to the node $j$ in $k$ steps of a random walk. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42012bd",
   "metadata": {
    "id": "vdylqNJVreik"
   },
   "source": [
    "Write a function `csr_transition_matrix` that takes a graph and returns a transition matrix. To save memory and speed up the following calculation of powers, convert transition matrix to sparse format by `scipy.sparse.csr_matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb3f134",
   "metadata": {
    "deletable": false,
    "id": "ijUKHWFDLKUc",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bec7a132a8e91292a33f9ce803d9e648",
     "grade": false,
     "grade_id": "cell-f0ec11b1c4da2c21",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def csr_transition_matrix(G):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60607b89",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "414367d3e3ea4d608a89daa8c46d77ab",
      "64425f50f6c64e71b36a84d9dc00b296",
      "81dd316a820a4dd98f5770241b14fc56",
      "4b9a2e5ed5ca4a9aaac9f7ff44a7c84b",
      "468e0e0c7e5b4b24a7f180e9b177d377",
      "c0961ed13ab549719b896b6e19885e4e",
      "e21f49b6078648ad9d58fb962700db3d",
      "b551e27a37784378aab301cd6b481b3a",
      "10c0fb90ee814efc8180bb7a48e7305d",
      "7f0d551f9ac646d987cb2f198fce7c79",
      "1ee478f0cef24e9a8003e14f48efb787"
     ]
    },
    "deletable": false,
    "editable": false,
    "id": "LKqHuKwhLvI6",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "89c5d34c88f598b589ecaf7c68858755",
     "grade": true,
     "grade_id": "cell-2d2f99d8bac11893",
     "locked": true,
     "points": 0.75,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "8651634b-040b-45f9-df47-88f2cdb64dad"
   },
   "outputs": [],
   "source": [
    "P = csr_transition_matrix(G)\n",
    "assert type(P) == csr_matrix\n",
    "assert P.shape == (len(G), len(G))\n",
    "assert round(P[3, 9], 4) == 0.0417"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdf780c",
   "metadata": {
    "id": "NAQLGKBAH8tf"
   },
   "source": [
    "Similar to DeepWalk, there are two types of embeddings:\n",
    "* $v_i$ is a vector that represents the start node $i$\n",
    "* $u_j$ is a vector that represents the context node $j$\n",
    "\n",
    "Our objective aims to maximize: 1) the probability that these pairs come from the graph, and 2) the probability that all other pairs do not come from the graph (negative samples). The objective function for nodes $i, j$ in the transition matrix of the power $k$ is\n",
    "\n",
    "$$\\mathcal L^k_{ij} = P^k_{ij}\\log \\sigma(v_i^\\top u_j) + \\frac{\\lambda}{|V|}\\sum_{t=1}^{|V|}P^k_{tj}\\log \\sigma(-v_t^\\top u_j)$$\n",
    "\n",
    "where $\\lambda$ is the hyperparameter indicating the number of negative examples. Optimizing this objective makes observed pairs $(i, j)$ have similar embeddings, while scattering unobserved pairs $(t, j)$. Substitution $v_i^\\top u_j = X_{ij}$ and setting $\\frac{\\partial \\mathcal L_{ij}}{ \\partial X_{ij}} = 0$ yields\n",
    "\n",
    "$$v_i^\\top u_j = X_{ij}^k = \\log \\frac{P^k_{ij}}{\\sum_{t=1}^{|V|}P^k_{tj}} - \\log\\frac{\\lambda}{|V|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e123103",
   "metadata": {
    "id": "X7bwqmWGs-QC"
   },
   "source": [
    "Let us call $X^k$ log probabilistic matrix. Write a function `log_probabilistic_matrix` that takes a transition matrix, hyperparameter $\\lambda$ and returns a log probabilistic matrix.\n",
    "\n",
    "_Remark: to prevent $-\\infty$ in log, add $\\varepsilon=1^{-6}$ as follows `np.log(x + 1e-6)`_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efddf0a",
   "metadata": {
    "deletable": false,
    "id": "XjVAyoUiMq_8",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "63684003d32189034bf77a52da99cd2b",
     "grade": false,
     "grade_id": "cell-c949fe784127c31a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def log_probabilistic_matrix(P, lambd):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714cea5c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "eaf532492cc2425db11cd67e6ec23ebf",
      "b77c5b29a7104db48413aad1c50b5aae",
      "7b1e5ada2ca24915a1619821e94cce93",
      "78bafc3e493c41f6ae29ede92e1a05da",
      "681af90fa8054b58a1940341f6d801fa",
      "466f48d279ad420aa72838443116fad6",
      "1a8116ec5bc84bb49410fa2e86c16f2f",
      "4646ac45ffcc48f29e6b5660d78d855d",
      "9735cb7492e74a6f97966088ca84a665",
      "6f934c75474a45fa8ba082cadd70015e",
      "d0ecb092472447e18eeb6567d200ba52"
     ]
    },
    "deletable": false,
    "editable": false,
    "id": "KTl_DISFt8mA",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f81afad0435cd21fc8814f7974fd775b",
     "grade": true,
     "grade_id": "cell-624f6d1384718d6f",
     "locked": true,
     "points": 0.75,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "ec96a030-10f9-4591-adc7-254174c0227b"
   },
   "outputs": [],
   "source": [
    "X = log_probabilistic_matrix(P, lambd=5)\n",
    "assert type(X) == np.matrix\n",
    "assert X.shape == (len(G), len(G))\n",
    "assert round(X[3, 9], 4) == 3.182"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cf4400",
   "metadata": {
    "id": "y56r1-evIBAJ"
   },
   "source": [
    "The log probabilistic matrix $X^k$ can be approximated by truncated SVD so that\n",
    "\n",
    "$$X^k \\approx X_d^k = U_d^k \\Sigma_d^k (V_d^k)^\\top$$\n",
    "\n",
    "where $d$ is the number of singular values of truncated SVD. Thus, embeddings of start nodes can be obtained by \n",
    "\n",
    "$$v_i = \\left[U_d^k \\sqrt{\\Sigma_d^k}\\right]_i$$ \n",
    "\n",
    "and embeddings of context nodes by \n",
    "\n",
    "$$u_j = \\left[\\sqrt{\\Sigma_d^k} (V_d^k)^\\top\\right]_j$$ \n",
    "\n",
    "_Remark: the power 1/2 makes SVD symmetric. It is empirically shown that the symmetric SVD is better for social graphs and makes embeddings more \"similar\" to corresponding embeddings of SkipGram with negative sampling in a sence of some matrix properties._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de811aed",
   "metadata": {
    "id": "M5gDGuACuy-5"
   },
   "source": [
    "Write a function `svd_emb` that takes a log probabilistic matrix, number of dimensions (number of largest singular values) in truncated SVD and returns embeddings of start nodes.\n",
    "\n",
    "_Hint: use `scipy.sparse.linalg.svds` to calculate truncated SVD for a given number of singular values._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1639e192",
   "metadata": {
    "deletable": false,
    "id": "S9yoD3FxOXbw",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "64aa1dc62ca5b5035ec1eedfc7372ab7",
     "grade": false,
     "grade_id": "cell-683cb8f26df5fd3a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def svd_emb(X, dim):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6c0a04",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b79ac0db377db5bae1f2923b7877e2dc",
     "grade": true,
     "grade_id": "cell-426bb9037fe96aba",
     "locked": true,
     "points": 0.75,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "_emb = svd_emb(X, dim=4)\n",
    "assert type(_emb) == np.ndarray\n",
    "assert _emb.shape == (3873, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f214ef",
   "metadata": {
    "id": "DMgCQ_9GxjKg"
   },
   "source": [
    "\n",
    "The resulting node embeddings are concatenated from the log probabilistic matrices $X, X^2, ..., X^k$.\n",
    "\n",
    "In `grarep_emb`, we calculate powers of transition matrix $P, P^2, \\dots, P^k$, calculate and concatenate embeddings of start nodes, then we compress them to the original number of dimensions by PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69051b70",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "08744ba53988cb545f953cce607f67c6",
     "grade": false,
     "grade_id": "cell-fe1d6156e8d730ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def grarep_emb(G, dim, k, lambd):\n",
    "    emb = []\n",
    "    P_1 = csr_transition_matrix(G)\n",
    "    P = P_1.copy()\n",
    "    for i in trange(k):\n",
    "        X = log_probabilistic_matrix(P, lambd=lambd)\n",
    "        _emb = svd_emb(X, dim=dim)\n",
    "        emb.append(_emb)\n",
    "        if i == (k - 1):\n",
    "            continue\n",
    "        P = P @ P_1\n",
    "    emb = PCA(n_components=dim).fit_transform(np.concatenate(emb, axis=1))\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09dc038",
   "metadata": {},
   "source": [
    "Finally, we evaluate the model by mutual information between ground truth labels and cluster indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df44883f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "9313e90778c04cb9be8038087f8de9e4",
      "742d688b293c4523bda4abd2d3d9d203",
      "2b20519ff4354656942d63d067c61740",
      "6f5f386cc8ce4b70aa5b3a2202cdfb74",
      "5c3c0e1550544177bd063261ffe8ca12",
      "0d9b6e664d9c4a1bbe20eb7fdf16da2f",
      "092ec69a30964666bd92896978c0c976",
      "4234c3dd32264885ba8b7e7724a2235a",
      "4104856a8c704b85be422c3baae777ed",
      "7bf3033f9a50479289d5980a1ff22cd6",
      "b771aabb1ac74a7fbdf94c8f0bd796b9"
     ]
    },
    "deletable": false,
    "editable": false,
    "id": "0NvbTouIFSm7",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a8be8f00f7fc401b45409cd399e91d28",
     "grade": true,
     "grade_id": "cell-a016b73d556e34c9",
     "locked": true,
     "points": 0.75,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "b8d7c630-5f5d-4439-acfa-bdb4b06be7c7"
   },
   "outputs": [],
   "source": [
    "emb = grarep_emb(G, dim=16, k=4, lambd=5)\n",
    "assert emb.shape == (len(G), 16)\n",
    "_, pred_labels, _ = k_means(emb, n_clusters=8)\n",
    "mi = mutual_info_score(labels, pred_labels)\n",
    "assert mi > 0.22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43bd7b6",
   "metadata": {
    "id": "VkLVoqDV0L93"
   },
   "source": [
    "Plot t-SNE visualization of node embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aa3ef3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 499
    },
    "id": "ur-bWmtED-DE",
    "outputId": "87a5956c-dbbf-42fe-c351-123a04d6f41a"
   },
   "outputs": [],
   "source": [
    "decomposition = TSNE(n_components=2)\n",
    "xy_emb = decomposition.fit_transform(emb)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "scatter = plt.scatter(xy_emb[:, 0], xy_emb[:, 1], c=labels, s=10, cmap=plt.cm.Set2)\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=unique)\n",
    "plt.title(f'MI: {mi:.4f}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e85d50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
